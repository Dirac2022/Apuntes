- **Curso:** [Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization | Coursera](https://www.coursera.org/learn/deep-neural-network?specialization=deep-learning) por Andrew Ng en Coursera.


	- Playlist: [Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization (Course 2 of the Deep Learning Specialization) - YouTube](https://www.youtube.com/playlist?list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc)
	- Resource: [deep-learning-specialization/C2-Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization at main Â· greyhatguy007/deep-learning-specialization](https://github.com/greyhatguy007/deep-learning-specialization/tree/main/C2-Improving%20Deep%20Neural%20Networks%3A%20Hyperparameter%20Tuning%2C%20Regularization%20and%20Optimization)

# [[Practical Aspects of Deep Learning]]

## Videos

- [ ] 1. Train / Dev / Test sets
- [ ] 2. Bias / Variance
- [ ] 3. Basic Recipe for Machine Learning
- [ ] 4. Regularization
- [ ] 5. Why Regularization Reduces Overfitting?
- [ ] 6. Dropout Regularization
- [ ] 7. Understanding Dropout
- [ ] 8. Other Regularization Methods
- [ ] 9. Normalizing Inputs
- [ ] 10. Vanishing / Exploding Gradients
- [ ] 11. Weight Initialization for Deep Networks
- [ ] 12. Numerical Approximation of Gradient
- [ ] 13. Gradient Checking
- [ ] 14. Gradient Checking Implementation Notes
- [ ] 15. Yoshua Bengio Interview


## Reading
- [ ] Clarification about Upcoming Regularization Video (1 minute)
- [ ] Clarification about Upcoming Understanding Dropout Video (1 minute)
- [ ] Lecture Notes W1 (1 minute)

## Assignments (Practice quiz)
- [ ] Practical Aspects of Deep Learning (50 minutes)
- [ ] 
## Programming assignment
- [ ] Initialization (180 minutes)
- [ ] Regularization (180 minutes)
- [ ] Gradient Checking (180 minutes)



# [[Optimization Algorithms]]

## Videos

- [ ] 16. Mini-batch Gradient Descent
- [ ] 17. Understanding Mini-batch Gradient Descent
- [ ] 18. Exponentially Weighted Averages
- [ ] 19. Understanding Exponentially Weighted Averages
- [ ] 20. Bias Correction in Exponentially Weighted Averages
- [ ] 21. Gradient Descent with Momentum
- [ ] 22. RMSprop
- [ ] 23. Adam Optimization Algorithm
- [ ] 24. Learning Rate Decay
- [ ] 25. The Problem of Local Optima
- [ ] 26. Yuanqing Lin Interview

## Reading
- [ ] Clarification about Upcoming Adam Optimization Video(1 minutes)
- [ ] Clarification about Learning Rate Decay Video (1 minute)
- [ ] Lecture Notes W2 (1 minutes)

## Assignments 
- [ ] Optimization Algorithms (50 minutes)
## Programming assignment
- [ ] Optimization Methods (180 minutes)


# [[Hyperparameter Tuning, Batch Normalization and Programing Frameworks]]

## Videos

- [ ] 27. Tuning Process
- [ ] 28. Using an Appropriate Scale to pick Hyperparameters
- [ ] 29. Hyperparameters Tuning in Practice: Pandas vs. Caviar
- [ ] 30. Normalizing Activations in a Network
- [ ] 31. Fitting Batch Norm into a Neural Network
- [ ] 32. Why does Batch Norm work?
- [ ] 33. Batch Norm at Test Time
- [ ] 34. Softmax Regression
- [ ] 35. Training a Softmax Classifier
- [ ] 36. Deep Learning Frameworks
- [ ] 37. TensorFlow


## Reading
- [ ] Clarification about Upcoming Normalizing Activations in a Network Video (1 minute)
- [ ] Clarifications about Upcoming Softmax Video (1 minute)
- [ ] (Optional) Learn about Gradient Tape and More (1 minute)
- [ ] Lecture Notes W3 (1 minute)

## Assignments
- [ ] Hyperparameter tuning, Batch Normalization, Programming Frameworks (50 minutes)

## Programming assignment
- [ ] TensorFlow Introduction (180 minutes)
