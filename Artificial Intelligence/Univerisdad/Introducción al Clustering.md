
**Aprendizaje Supervisado**
```mermaid
graph LR

A["Datos de entrada (X) + Etiquetas (Y)"] --> B["Mapeo entrada -> Salida"]
```
- Regresi√≥n
- Clasificaci√≥n
- Predicci√≥n

**Aprendizaje no Supervisado**
```mermaid
graph LR
A["Solo datos de entrada (X)"] --> B["Encuentra patrones/Estructuras"]
```
- CLustering
- Reducci√≥n de dimensionalidad
- Detecci√≥n de anomal√≠as


# Clustering

El **Clustering** o agrupamiento busca dividir un conjunto de datos en grupos o **cl√∫steres**, de manera que:
- Puntos dentro de un mismo cl√∫ster son **muy similares entre s√≠**
- Puntos en diferentes cl√∫steres son **diferentes unos de otros**

![[Pasted image 20250528120948.png]]

**¬øPara qu√© sirve el Clustering?

**üõí Segmentaci√≥n de clientes**
Agrupar clientes con comportamientos de compra similares para marketing personalizado.
> [!tip] üí° Ejemplo: Estrategias de marketing espec√≠ficas para cada perfil de cliente en e commerce

**‚ö†Ô∏è Detecci√≥n de Anomal√≠as**
Identificar puntos que no pertenecen a ning√∫n cl√∫ster o forman grupos muy peque√±os y aislados.
> [!tip] üí° Ejemplo: Identificaci√≥n de transacciones fraudulentas en sistemas bancarios



**üñºÔ∏è An√°lisis de Im√°genes**
Segmentaci√≥n de im√°genes, agrupando p√≠xeles o regiones con caracter√≠sticas similares.
> [!tip] üí° Ejemplo: Identificaci√≥n autom√°tica de objetos en visi√≥n por computadora


**üß¨ Bioinform√°tica**
Agrupar genes o prote√≠nas con funciones similares para analizar patrones biol√≥gicos.
> [!tip] üí° Ejemplo: Clasificaci√≥n de secuencias gen√©ticas para investigaci√≥n m√©dica


**üóÑÔ∏è Organizaci√≥n de datos**
Agrupar documentos, archivos multimedia o cualquier tipo de contenido con caracter√≠sticas similares.
> [!tip] üí° Ejemplo: Sistemas de recomendaci√≥n de m√∫sica basados en preferencias de escucha


# Tipos principales de Algoritmos de Clustering

%% Ampliar esta parte%%

### üéØ Basados en centroides
**K-means**
Particionan datos asignando puntos al centroide m√°s cercano

- R√°pido y eficiente
- Sensible a la inicializaci√≥n


### Basado en densidad
**DBSCAN**
Definen cl√∫steres como √°reas densas separadas por √°reas menos densas
- Detecta formas arbitrarias
- Dificultad con densidades variables

### Basado en conectividad
**Jer√°rquica**
Construyen una jerarqu√≠a de cl√∫steres visualizada como dendrograma
- Aglomerativo
- Divisivo
### Basado en modelos
**GMM (EM)**
Asumen que los datos provienen de una distribuci√≥n subyacente
- Soft clustering (probabil√≠stico)
- Sensible a inicializaci√≥n

### Basados en cuadr√≠culas
**STING, WaveCluster**
Dividen el espacio de datos en celdas formando una cuadr√≠cula
- R√°pido procesamiento
- Limitado por granularidad



# K-means

- Un m√©todo de partici√≥n que agrupa datos en K-cl√∫steres basados en centroides
- Algoritmo de clustering basado en centroides que agrupa objetos en K grupos predefinidos.

- Divide datos en **K cl√∫steres** predefinidos
- Minimiza la **distancia intra-cl√∫ster** entre puntos y su centroide
- Proceso **iterativo**: asignar puntos, recalcular centroides
- Utiliza **distancia Euclidiana** para medir similitud
- **Objetivo**: agrupar puntos similares en el mismo cl√∫ster


### Algoritmo K-means


1. **Inicializaci√≥n**
	Elegir el n√∫mero de cl√∫steres k y seleccionar k centroides iniciales (generalmente de forma aleatoria).
2. **Asignaci√≥n**
	A cada punto se le asigna el cl√∫ster cuyo centroide es el m√°s cercano, usando la distancia Euclidiana al cuadrado.
3. **Actualizaci√≥n**
	Calcular la nueva posici√≥n de cada centroide como la media de todos los puntos asignadas a ese cl√∫ster.
4. **Iteraci√≥n**
	Repetir los pasos de **asignaci√≥n** y **actualizaci√≥n** hasta que los centroides no cambien significativamente o se alcance un m√°ximo de iteraciones



```mermaid
flowchart TD
    A[Inicializar K centroides] --> B[Asignar puntos al centroide m√°s cercano]
    B --> C[Actualizar posici√≥n de centroides]
    C --> D{¬øConverge?}
    D -- No --> B
    D -- S√≠ --> E[Criterio de parada]
    E --> F[Cl√∫steres finales]

    %% Estilo opcional
    style A fill:#c084fc,color:white
    style B fill:#a78bfa,color:white
    style C fill:#8b5cf6,color:white
    style D fill:#facc15,color:black
    style E fill:#10b981,color:white
    style F fill:#22d3ee,color:black

```

### Visualizaci√≥n interactiva del proceso K-means

<img src="https://stanford.edu/~cpiech/cs221/img/kmeansViz.png" >
[CS221](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html)


1. Centroides iniciales
2. Asignaci√≥n de puntos
3. Actualizaci√≥n de centroides
4. Reasignaci√≥n
5. Estado final

### M√©todo del codo (Elbow method)
T√©cnica para determinar el n√∫mero √≥ptimo de cl√∫steres ($k$) calculando la **suma de cuadrados dentro del cl√∫ster (wcss)** para diferentes valores de $k$.

El punto donde la disminuci√≥n de WCSS se ralentiza significativamente, formando un "codo" en la gr√°fica, sugiere un buen valor para $k$.

$$
	\text{WCSS} = \sum_{i=1}^k \sum_{x \in C_i} \|x - \mu_i \|^2
$$
Donde $\mu_i$ es el centroide del cl√∫ster $C_i$

### Otras consideraciones

**El m√©todo del codo es una heur√≠stica**
No proporciona una respuesta definitiva. Es una gu√≠a visual que puede ser ambigua cuando no hay  un "codo" claro en la gr√°fica.

**An√°lisis de Silueta**
T√©cnica alternativa que mide qu√©  tan similar es un punto a su propio cl√∫ster comparado con otros
 cl√∫steres. Un alto valor de silueta indica buena cohesi√≥n y separaci√≥n entre cl√∫steres.

**Criterio de Negocio**
 A veces, la elecci√≥n de K depende de la interpretabilidad y utilidad pr√°ctica de los grupos. Considera los requisitos del problema y el contexto de aplicaci√≥n.


### Ventajas

 - Simple y f√°cil de implementar
 - Eficiente computacionalmente
 - **Escalable**. Funciona bien con grandes conjuntos de datos y es
 adaptable a diferentes tama√±os

### Limitaciones
- **Sensible a la inicializaci√≥n**.  Diferentes puntos de partida pueden llevar a diferentes
 resultados finales.
- Requiere especificar $k$
- **Asume cl√∫steres convexos y similares**. No funciona bien con formas irregulares o densidades variables
- Sensible a outliers
- Requiere datos num√©ricos



# Self-Organizing Maps (SOM)

- Los Self-Organizing Maps (SOM), tambi√©n conocidos como mapas de Kohonen, son una forma de red neuronal artificial para aprendizaje no supervisado.
	
- Su objetivo principal es mapear datos de alta dimensi√≥n a un espacio de baja dimensi√≥n (t√≠picamente una cuadr√≠cula 2D). 
	
- La caracter√≠stica m√°s importante es que preservan la topolog√≠a: puntos cercanos en el espacio original permanecen cercanos en el mapa resultante. 
	
- Se consideran una forma de "K-means restringido" o "cuantizaci√≥n vectorial" y son excelentes para la visualizaci√≥n de datos complejos.

### Estructura

Un Self-Organizing Map (SOM) consta de: 
- **Capa de entrada**: Tiene tantas dimensiones como los datos de entrada. 
- **Cuadr√≠cula de salida 2D**: Un arreglo bidimensional de neuronas (ejemplo: 5x5, 10x10). 
- **Vectores de pesos**: Cada neurona en la cuadr√≠cula tiene un vector de pesos de la misma dimensi√≥n que los datos de entrada.
- Inicialmente, estos pesos se asignan de forma aleatoria y se van ajustando durante el entrenamiento.

<img src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*0PdY0c_2FFZ1-BY-_j0yRA.png" >
[Self Organizing Map(SOM) with Practical Implementation | by Amir Ali | Aorb Tech | Medium](https://medium.com/machine-learning-researcher/self-organizing-map-som-c296561e2117)


### Entrenamiento de un SOM

1. **Inicializaci√≥n**. Los pesos de todas las neuronas en la cuadr√≠cula se inicializan aleatoriamente con valores en el rango de los datos de entrada.
	
2. **Selecci√≥n de datos**. Se selecciona un punto de dato de entrenamiento aleatoriamente del conjunto de datos.
	
3. **Encontrar la BMU**. Se identifica la Best Matching Unit (BMU), la neurona cuyo vector de pesos es m√°s similar al punto de dato (usando distancia euclideana).
	
4. **Actualizaci√≥n de pesos**. Se ajustan los pesos de la BMU y sus vecinos, movi√©ndose ligeramente hacia el punto de dato. La tasa de aprendizaje y el radio del vecindario disminuyen con el tiempo.
	
5. **Iteraci√≥n**. Se repiten los pasos 2-4 miles de veces con diferentes puntos de datos hasta que los pesos converjan a una representaci√≥n estable.


### Convergencia e Issues de SOM

**Convergencia**
El proceso de entrenamiento de un SOM requiere muchas iteraciones para converger a una representaci√≥n estable de los datos. A medida que avanza el entrenamiento, tanto la tasa de aprendizaje como el radio del vecindario disminuyen gradualmente.

**Variabilidad en resultados**
Los SOM b√°sicos pueden generar resultados variables (diferentes mapeos) cuando se ejecutan varias veces, debido a la inicializaci√≥n aleatoria y la naturaleza estoc√°stica del entrenamiento

**Soluciones**
Existen variaciones y extensiones para mitigar estos problemas, como ejecutar el algoritmo varias veces y fusionar los resultados, o usar t√©cnicas de inicializaci√≥n de pesos m√°s sofisticadas.


### Aplicaciones

**Visualizaci√≥n de datos**
Principal fortaleza de SOM: mapeo de datos de alta dimensi√≥n a una representaci√≥n 2D visual que preserva relaciones topol√≥gicas.

**An√°lisis exploratorio**
Permite descubrir patrones, relaciones y estructuras ocultas en datos complejos sin necesidad de especificar K como en K-means.


>[!tip] Punto clave
>Aunque los SOM pueden usarse indirectamente para clustering (agrupando puntos en la misma neurona o en neuronas cercanas), su mayor valor est√° en la visualizaci√≥n e interpretaci√≥n de datos complejos.

### K-means vs SOM

| Caracter√≠stica          | K-means                                                                 | SOM                                                                 |
|-------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------|
| **Objetivo**            | Minimizar la suma de cuadrados dentro del cl√∫ster (WCSS)                | Organizar la cuadr√≠cula preservando la topolog√≠a de los datos       |
| **Resultado**           | K cl√∫steres discretos con puntos asignados y centroides definidos       | Mapeo de datos a una representaci√≥n visual en cuadr√≠cula 2D         |
| **Ventaja Principal**   | Agrupamiento directo                                                    | Visualizaci√≥n                                                       |
| **Configuraci√≥n**       | R√°pido y escalable para clustering est√°ndar                             | Ideal para visualizaci√≥n y an√°lisis exploratorio de datos           |
| **Eficiencia**          | Simplicidad                                                             | Interpretabilidad, Exploraci√≥n                                      |
| **Sensibilidad**        | Requiere especificar K expl√≠citamente                                   | No requiere K expl√≠cito, pero s√≠ el tama√±o de la cuadr√≠cula         |
| **Limitaciones**        | Alto a formas de cl√∫steres no convexos y densidades variables           | Puede dar resultados variables por inicializaci√≥n aleatoria         |
|                         | Limitado a formas el√≠pticas                                             | Puede requerir varias ejecuciones                                   |

# Support Vector Machines

Las M√°quinas de Soporte Vectorial (SVM) son algoritmos de aprendizaje supervisado utilizados principalmente para clasificaci√≥n, especialmente binaria.

- Clasificaci√≥n binaria potente
- Encuentra el hiperplano √≥ptimo separador
- Alta efectividad en espacios de alta dimensi√≥n
- Extensible a m√∫ltiples clases y regresi√≥n


### El hiperplano separador y el margen

---


Perfecto. Te preparar√© una clase extensa y detallada sobre Support Vector Machines (SVM), desde sus fundamentos te√≥ricos y matem√°ticos hasta su implementaci√≥n en pseudoc√≥digo. Incluir√©:

- Una introducci√≥n conceptual a SVM.
    
- La formulaci√≥n matem√°tica (primal y dual) con el uso de Lagrangianos y explicaciones intuitivas accesibles para alguien con tu nivel de formaci√≥n.
    
- Una secci√≥n completa dedicada al SVD (Descomposici√≥n en Valores Singulares), explicando c√≥mo funciona, para qu√© sirve y c√≥mo se relaciona con SVM en ciertos contextos.
    
- Un an√°lisis profundo de los diferentes tipos de _kernel_, su interpretaci√≥n, formulaci√≥n matem√°tica y c√≥mo transforman los datos.
    
- Implementaci√≥n en pseudoc√≥digo paso a paso de SVM tanto para el caso lineal como con _kernel_.
    

Te avisar√© cuando el contenido est√© listo para tu revisi√≥n.

# M√°quinas de Vectores de Soporte (SVM)

## 1. Introducci√≥n

Las **m√°quinas de vectores de soporte** (Support Vector Machines, SVM) son algoritmos de aprendizaje supervisado para clasificaci√≥n (y tambi√©n para regresi√≥n). Una SVM busca un hiperplano que separe dos clases de datos con el **margen m√°ximo**, esto es, maximizando la distancia entre el hiperplano y los puntos m√°s cercanos de cada clase. De forma intuitiva, clasifica nuevos ejemplos ubic√°ndolos de un lado u otro de dicho hiperplano √≥ptimo. Las SVM fueron desarrolladas por Vladimir Vapnik y colaboradores en los a√±os 90. Al manejar un _margen amplio_ entre clases, suelen generalizar bien a nuevos datos. En la pr√°ctica las SVM se usan ampliamente en clasificaci√≥n binaria (y multiclase por extensiones) de datos lineales y no lineales.

En la pr√°ctica, cada punto de datos se representa como un vector en ‚Ñù^n. El hiperplano separador es de la forma _w¬∑x + b = 0_, donde **w** es un vector normal al hiperplano y _b_ es el sesgo. Las SVM lineales se usan cuando los datos son separables linealmente, y para datos no separables se aplican las **funciones kernel** que permiten transformar el espacio de forma impl√≠cita a alta dimensi√≥n. Los puntos de entrenamiento que quedan exactamente sobre el margen √≥ptimo se llaman **vectores de soporte**, y determinan el clasificador final.

## 2. Formulaci√≥n primal y dual del SVM

### 2.1 Caso de margen duro (primal)

En el caso ideal de datos totalmente separables, el objetivo es encontrar el hiperplano que maximiza el margen entre clases. Se demuestra que esto equivale a resolver el siguiente problema de optimizaci√≥n cuadr√°tica (sin t√©rminos de penalizaci√≥n de errores):

- **Problema primal (hard-margin SVM):**
    $$\begin{aligned} & \min_{w,b}\;\; \frac{1}{2}\|w\|^2, \\ & \text{sujeto a } y_i\,(w^T x_i + b) \;\ge\; 1,\quad i=1,\dots,m. \end{aligned}
$$

Aqu√≠ $x_i$ es el vector de caracter√≠sticas de la muestra _i_, $y_i\in{+1,-1}$ su etiqueta, y la condici√≥n $y_i(w^T x_i+b)\ge1$ asegura que cada punto se encuentre fuera o sobre el margen de separaci√≥n. Minimizar $\frac12|w|^2$ equivale a maximizar el margen (la distancia entre las l√≠neas paralelas $w^T x+b=\pm1$). Esta formulaci√≥n es convexa y puede ser resuelta mediante t√©cnicas de programaci√≥n cuadr√°tica.

### 2.2 Caso de margen suave (primal)

Cuando los datos no son completamente separables, se permite cierto error usando variables de holgura $\xi_i\ge0$ y un par√°metro de penalizaci√≥n $C>0$. El problema primal se modifica para penalizar violaciones del margen:

- **Problema primal (soft-margin SVM):**
    
    min‚Å°w,b,Œæ‚ÄÖ‚Ää‚ÄÖ‚Ää12‚à•w‚à•2+C‚àëi=1mŒæi,sujeto¬†a¬†yi‚Äâ(wTxi+b)‚ÄÖ‚Ää‚â•‚ÄÖ‚Ää1‚àíŒæi,Œæi‚â•0,‚ÄÖ‚Ääi=1,‚Ä¶,m. \begin{aligned} & \min_{w,b,\xi}\;\; \frac{1}{2}\|w\|^2 + C\sum_{i=1}^m \xi_i, \\ & \text{sujeto a } y_i\,(w^T x_i + b) \;\ge\; 1 - \xi_i,\quad \xi_i \ge 0,\; i=1,\dots,m. \end{aligned}

El t√©rmino $C\sum\xi_i$ penaliza la suma de errores, donde $C$ controla la rigidez del margen. Si $C$ es grande, se castigan fuertemente las violaciones (margin _casi_ duro); si $C$ es peque√±o, se permiten m√°s errores. En este caso, un punto con $\xi_i>0$ est√° mal clasificado o dentro del margen, reduciendo el ancho m√°ximo factible, pero se equilibra minimizando $|w|^2+C\sum\xi_i$. La formulaci√≥n sigue siendo convexa y admiten soluci√≥n por m√©todos de programaci√≥n cuadr√°tica.

### 2.3 Formulaci√≥n dual (Lagrangiana)

Para resolver estos problemas con m√©todos eficientes, se suelen derivar las **formulaciones duales** mediante multiplicadores de Lagrange. Definimos la funci√≥n Lagrangiana introduciendo multiplicadores $\alpha_i\ge0$ por cada restricci√≥n $y_i(w^Tx_i+b)\ge1-\xi_i$ (y $\mu_i\ge0$ para $\xi_i\ge0$). Por ejemplo, para el caso separable la Lagrangiana es:

L(w,b,Œ±)‚ÄÖ‚Ää=‚ÄÖ‚Ää12‚à•w‚à•2+‚àëi=1mŒ±i(1‚àíyi(wTxi+b)). L(w,b,\alpha) \;=\; \frac{1}{2}\|w\|^2 + \sum_{i=1}^m \alpha_i\big(1 - y_i(w^T x_i + b)\big).

Fijando las derivadas parciales $\partial L/\partial w = 0$ y $\partial L/\partial b = 0$, se obtiene que $w = \sum_i \alpha_i y_i x_i$ y $\sum_i \alpha_i y_i = 0$. Sustituyendo de nuevo y reorganizando, el problema dual resultante (maximizaci√≥n) es:

max‚Å°Œ±‚ÄÖ‚Ää‚ÄÖ‚Ää‚àëi=1mŒ±i‚àí12‚àëi,j=1mŒ±iŒ±j‚Äâyiyj‚ÄâxiTxj,sujeto¬†a¬†Œ±i‚â•0,‚ÄÖ‚Ää‚àëi=1mŒ±iyi=0. \begin{aligned} & \max_{\alpha}\;\; \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i,j=1}^m \alpha_i\alpha_j\,y_i y_j\,x_i^T x_j,\\ & \text{sujeto a } \alpha_i\ge0,\;\sum_{i=1}^m \alpha_i y_i = 0. \end{aligned}

Este es un problema de maximizaci√≥n c√≥ncava con restricciones lineales. En el caso de margen suave la √∫nica diferencia es que aparece la restricci√≥n superior $\alpha_i \le C$ en el dual, dando $0\le \alpha_i \le C$. En ambos casos el vector soluci√≥n es $w=\sum_i \alpha_i y_i x_i$, y la constante $b$ se obtiene de las condiciones de Karush‚ÄìKuhn‚ÄìTucker (KKT) para los vectores de soporte.

### 2.4 Condiciones de KKT y vectores de soporte

Las condiciones de Karush‚ÄìKuhn‚ÄìTucker (KKT) aplican a este problema convexo con restricciones. En particular, para cada punto de entrenamiento $i$ deben cumplirse (hard-margin):

Œ±i‚â•0,yi(wTxi+b)‚àí1‚â•0,Œ±i(yi(wTxi+b)‚àí1)=0. \alpha_i \ge 0,\quad y_i(w^T x_i + b)-1 \ge 0,\quad \alpha_i\big(y_i(w^T x_i + b)-1\big) = 0.

Esta **condici√≥n de complementariedad** implica que si $\alpha_i>0$ entonces $y_i(w^T x_i+b)=1$, lo que significa que el punto est√° _exactamente sobre el margen_. Tales puntos con $\alpha_i>0$ definen el margen y se conocen como **vectores de soporte**. Si $\alpha_i=0$, entonces $y_i(w^T x_i+b)>1$ (el punto est√° fuera del margen) y no influye en $w$. De modo similar, en el caso suave la KKT a√±ade condiciones an√°logas con las variables de holgura. En resumen, s√≥lo los vectores de soporte (pocos puntos) determinan la soluci√≥n final, y el hiperplano √≥ptimo se expresa como una combinaci√≥n lineal de ellos.

## 3. N√∫cleos (_Kernels_)

Las SVM lineales s√≥lo separan datos linealmente separables. Para datos no separables se utiliza el **truco del kernel**: se define una funci√≥n $K(x,z)$ que calcula el producto escalar en un espacio de caracter√≠sticas de dimensi√≥n alta (incluso infinita) sin mapear expl√≠citamente los datos. Formalmente, un _kernel_ es una funci√≥n sim√©trica $K:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ que cumple la condici√≥n de Mercer (la matriz de Gram $K_{ij}=K(x_i,x_j)$ es semidefinida positiva). Gracias a Mercer, existe una transformaci√≥n $\phi(x)$ tal que $K(x,z)=\langle\phi(x),\phi(z)\rangle$. De esta manera, la SVM resuelve el problema de separaci√≥n lineal en el espacio de caracter√≠sticas $\phi(x)$, sin calcular $\phi$ directamente, s√≥lo usando $K(x,z)$ en la formulaci√≥n dual.

Las funciones kernel m√°s comunes son:

- **Lineal:** $K(x,z)=x^T z$. Corresponde al caso original sin transformaci√≥n (espacio de caracter√≠sticas id√©ntico).
    
- **Polinomial:** $K(x,z)=(x^T z + \gamma)^d$, donde $d$ es el grado del polinomio y $\gamma\ge0$ es un t√©rmino de sesgo. Permite fronteras de decisi√≥n polinomiales de grado $d$.
    
- **Gaussiana RBF (Radial Basis Function):** $K(x,z)=\exp(-\gamma|x-z|^2)$, con $\gamma>0$. Mapea impl√≠citamente a un espacio infinito; produce fronteras muy flexibles basadas en distancia eucl√≠dea.
    
- **Sigmoide (tangente hiperb√≥lica):** $K(x,z)=\tanh(\kappa,x^T z + \theta)$, similar a una neurona de red neuronal. No siempre satisface estrictamente las condiciones de Mercer, pero funciona en la pr√°ctica.
    

|**Kernel**|**F√≥rmula**|**Par√°metros**|
|---|---|---|
|Lineal|$K(x,z)=x^T z$|‚Äì|
|Polinomial|$K(x,z)=(x^Tz+\gamma)^d$|$d$ (grado), $\gamma$|
|Gaussiano (RBF)|$K(x,z)=\exp(-\gamma\|x-z\|^2)$|$\gamma>0$|
|Sigmoide|$K(x,z)=\tanh(\kappa,x^Tz + \theta)$|$\kappa,\theta$|

Las SVM con kernel resultan muy potentes: por ejemplo, usando RBF se pueden separar datos no linealmente separables en el espacio original. Geom√©tricamente, el kernel define una medida de similitud; la SVM encuentra un hiperplano en el espacio de caracter√≠sticas con m√°ximo margen. Cada kernel induce un espacio de Hilbert distinto. La elecci√≥n del kernel (y sus par√°metros) se basa en la estructura de los datos y se suele hacer por validaci√≥n cruzada.

## 4. Descomposici√≥n en Valores Singulares (SVD)

### 4.1 ¬øQu√© es la SVD?

La **descomposici√≥n en valores singulares** (SVD) es una factorizaci√≥n de una matriz real $A\in\mathbb{R}^{m\times n}$ en tres matrices:

A‚ÄÖ‚Ää=‚ÄÖ‚ÄäUŒ£VT, A \;=\; U \Sigma V^T,

donde $U\in\mathbb{R}^{m\times m}$ y $V\in\mathbb{R}^{n\times n}$ son matrices ortonormales (sus columnas son vectores unitarios ortogonales), y $\Sigma\in\mathbb{R}^{m\times n}$ es diagonal (o bloque-diagonal) con los **valores singulares** $\sigma_1\ge \sigma_2\ge \cdots\ge 0$ de $A$ en la diagonal. Los valores singulares se obtienen de los autovalores de $A^T A$: si $A^T A$ tiene autovalores $\lambda_i$, entonces $\sigma_i=\sqrt{\lambda_i}$. En la SVD, las columnas de $V$ son los autovectores de $A^TA$, y las columnas de $U$ se obtienen normalizando $Av_i=\sigma_i u_i$.

Toda matriz $A$ admite una SVD (teorema garantizado). Los valores singulares reflejan la ‚Äúenerg√≠a‚Äù o varianza de $A$ en cada direcci√≥n. Una forma reducida (econ√≥mica) de SVD usa s√≥lo los primeros $r$ vectores singulares (rango de $A$).

### 4.2 Aplicaciones en reducci√≥n de dimensionalidad y SVM

La SVD tiene muchas aplicaciones en ML y ciencias de datos, especialmente en reducci√≥n de dimensionalidad. En PCA (An√°lisis de Componentes Principales), se usa la SVD de la matriz de datos centrada para obtener nuevas variables (componentes principales) que explican la mayor varianza. Al proyectar los datos originales sobre los primeros componentes principales, se reduce la dimensi√≥n preservando la mayor parte de la variaci√≥n. En palabras sencillas: ‚Äúla SVD se usa para reducir la dimensionalidad de los datos y proyectarlos a un espacio de menor dimensi√≥n‚Äù. Por ejemplo, PCA puede combinar variables correlacionadas en una sola componente principal, reduciendo ruido y redundancia. Otras aplicaciones incluyen la pseudoinversa de matrices (resoluci√≥n de sistemas sobredeterminados) y el An√°lisis Sem√°ntico Latente (LSA) en procesamiento de texto, que emplea SVD para descubrir relaciones sem√°nticas.

En el contexto de SVM, la SVD es relevante como t√©cnica de preprocesamiento. Si el conjunto de datos original tiene alta dimensi√≥n, primero se puede aplicar PCA (SVD) para reducir la dimensi√≥n efectiva y eliminar ruido antes de entrenar la SVM. Esto acelera el entrenamiento y puede mejorar la generalizaci√≥n. Asimismo, en m√©todos de SVM no lineal basados en kernels, existe una analog√≠a: el **kernel PCA** obtiene direcciones principales en el espacio de caracter√≠sticas definido por el kernel. Dicho de otro modo, la matriz de Gram del kernel $K$ es semidefinida positiva y puede descomponerse (autovectores o SVD), revelando las componentes principales en el espacio de Hilbert. As√≠, la SVD sirve para entender y procesar datos incluso cuando se usan kernels, pues todas las operaciones dependen de productos escalares.

## 5. Pseudoc√≥digo de implementaci√≥n de SVM

A continuaci√≥n se esquematizan los pasos b√°sicos para implementar un clasificador SVM binario (en la pr√°ctica se usar√≠a una librer√≠a o solver). Asumimos etiquetas $y_i\in{+1,-1}$:

1. **Preprocesamiento de los datos:**
    
    - Normalizar o escalar las caracter√≠sticas (por ejemplo, media cero y varianza unidad) para evitar sesgos en la optimizaci√≥n.
        
    - (Opcional) Transformar etiquetas al formato requerido ($\pm1$).
        
    - Dividir datos en conjunto de entrenamiento y prueba.
        
2. **Construir la matriz de kernel (Gram):**
    
    - Elegir funci√≥n kernel $K(x,z)$ (lineal, RBF, etc.).
        
    - Calcular $K_{ij} = K(x_i,x_j)$ para todas las parejas de puntos de entrenamiento.
        
3. **Resolver el problema dual:**
    
    - Definir las variables duales ${\alpha_i}$.
        
    - Aplicar un m√©todo QP. Por ejemplo, **SMO (Sequential Minimal Optimization)**: iterativamente elegir dos multiplicadores $\alpha_1,\alpha_2$ que violan las condiciones KKT, y optimizar anal√≠ticamente ese par. Repetir hasta que todos los $\alpha_i$ satisfagan la KKT (dentro de tolerancia). SMO es ampliamente usado (implementado en LIBSVM, etc.).
        
    - Al final se obtienen las soluciones $\alpha_i^*$. Las que quedan no nulas corresponden a vectores de soporte.
        
4. **Calcular par√°metros del modelo:**
    
    - **Vector de pesos:** En el caso lineal, calcular $w = \sum_i \alpha_i^* y_i x_i$. Si se usa kernel, no se calcula $w$ expl√≠cito en el espacio original, pero para predecir basta con las $\alpha_i^*$.
        
    - **T√©rmino independiente $b$:** Determinar $b$ a partir de cualquier vector de soporte (por ejemplo, promediar $y_i - w^T x_i$ sobre los que $0<\alpha_i<C$).
        
5. **Predicci√≥n de nuevas muestras:**
    
    - Dado un nuevo punto $x$, calcular la funci√≥n de decisi√≥n:  
        f(x)‚ÄÖ‚Ää=‚ÄÖ‚Ää‚àëiŒ±i‚àóyi‚ÄâK(xi,x)‚ÄÖ‚Ää+‚ÄÖ‚Ääb.f(x) \;=\; \sum_i \alpha_i^* y_i\,K(x_i,x)\;+\;b.
        
    - Asignar la clase $\operatorname{sign}(f(x))$. Equivalentemente, para SVM lineal $f(x)=w^T x+b$. Un valor positivo indica clase +1, y negativo clase -1.
        

En resumen, el entrenamiento SVM consiste en maximizar el dual sujeto a las restricciones (usando, p.ej., SMO), y luego usar los multiplicadores √≥ptimos para clasificar. La caracter√≠stica clave es que s√≥lo los vectores de soporte ($\alpha_i>0$) influyen en la soluci√≥n final, lo que hace al modelo eficiente en memoria y c√°lculo.

**Referencias:** Se ha citado ampliamente la literatura t√©cnica para cada concepto, incluyendo introducciones de IBM, notas de curso y tutoriales sobre SVM, y material sobre kernels y SVD. Estos recursos explican en detalle las f√≥rmulas, condiciones KKT y aplicaciones se√±aladas en esta clase.

