
# üéì Minicurso de PCA - Secci√≥n 1: Fundamentos Te√≥ricos

---

## 1.1 ¬øQu√© es PCA y para qu√© sirve?

**PCA (Principal Component Analysis)** es un m√©todo para **reducir la dimensionalidad** de un conjunto de datos **manteniendo tanta informaci√≥n como sea posible**.

- **Problema:** Datos de alta dimensi√≥n son dif√≠ciles de visualizar y manejar (maldita "maldici√≥n de la dimensionalidad").
- **Idea:** Encontrar nuevas coordenadas (ejes) en los que los datos se "extienden" m√°s.
- **Resultado:** Nuevas variables (componentes principales) que son combinaciones lineales de las originales.

**Ejemplo pr√°ctico:**
- Tienes datos con 100 caracter√≠sticas ‚Üí PCA puede reducirlos a 2 o 3 **capturando casi toda la informaci√≥n**.

---

## 1.2 Intuici√≥n geom√©trica: ¬øQu√© hace PCA?

Visualiza cada dato como un punto en el espacio.

- PCA encuentra las **direcciones** (l√≠neas, planos, etc.) donde los puntos est√°n m√°s **esparcidos**.
- Luego, **proyecta** los puntos sobre esas direcciones.
- As√≠, representas los datos **en menos dimensiones** con **m√≠nima p√©rdida de informaci√≥n**.

**Visualmente:**
- Imagina una nube de puntos en 3D.
- Hay una direcci√≥n donde los puntos est√°n m√°s dispersos.
- PCA te da esa direcci√≥n como el **primer componente principal**.

---

## 1.3 El papel de la varianza

**Varianza = medida de dispersi√≥n.**

- PCA busca **maximizar la varianza**: busca las direcciones donde los datos var√≠an m√°s.
- Si los datos var√≠an mucho en una direcci√≥n, probablemente esa direcci√≥n contiene **informaci√≥n importante**.

**Por eso:**
- El **primer componente principal** es la direcci√≥n de m√°xima varianza.
- El **segundo componente principal** es la direcci√≥n **ortogonal** a la primera que captura la segunda mayor varianza.
- Y as√≠ sucesivamente.

---

## 1.4 Un poco de √Ålgebra Lineal: Covarianza, Autovectores y Autovalores

### 1.4.1 Matriz de Covarianza

La **matriz de covarianza** $C$ mide c√≥mo var√≠an juntas dos variables.
Para un conjunto de datos centrado $X$ (es decir, datos con media cero en cada columna):

$$
C = \frac{1}{n-1} X^T X
$$

Donde:
- $X$ es la matriz de datos de tama√±o $n \times d$ (n muestras, d caracter√≠sticas),
- $X^T$ es la traspuesta de $X$.

Cada elemento $C_{ij}$ es la covarianza entre la caracter√≠stica $i$ y la caracter√≠stica $j$.

---

### 1.4.2 Autovectores y Autovalores

Recordatorio:
- Un **autovector** de una matriz $A$ es un vector $v$ tal que:

$$
A v = \lambda v
$$

- donde $\lambda$ es su **autovalor**.

**En PCA:**
- Los **autovectores** de la matriz de covarianza son las **direcciones principales** (componentes principales).
- Los **autovalores** nos dicen **cu√°nta varianza** hay en cada direcci√≥n.

‚úÖ **Ordenamos los autovectores seg√∫n los autovalores de mayor a menor.**

---

## 1.5 Conexi√≥n con el Teorema Espectral

Recordemos:  
- El **Teorema Espectral** dice que toda **matriz sim√©trica real** (como la matriz de covarianza) se puede diagonalizar:

$$
C = Q \Lambda Q^T
$$

donde:
- $Q$ tiene los autovectores ortonormales en sus columnas,
- $\Lambda$ es una matriz diagonal de autovalores.

**PCA b√°sicamente usa esta descomposici√≥n para encontrar las mejores direcciones de proyecci√≥n.**

---

# üìã En resumen (Secci√≥n 1)

- PCA busca **nuevos ejes** donde los datos est√°n m√°s dispersos.
- Estos ejes son los **autovectores** de la matriz de **covarianza** de los datos.
- Los **autovalores** nos dicen **cu√°nta informaci√≥n (varianza)** hay en cada direcci√≥n.
- Gracias al **Teorema Espectral**, sabemos que estos autovectores forman una base ortonormal.

---

# üéì Minicurso de PCA - Secci√≥n 2: Derivaci√≥n Matem√°tica Paso a Paso

---

## 2.1 Primer paso: centrar los datos

Antes de hacer PCA, **siempre debes centrar** los datos:
- **Centrar** significa hacer que cada variable tenga **media cero**.

**F√≥rmula:**
Si $X \in \mathbb{R}^{n \times d}$ es la matriz de datos (n muestras, d caracter√≠sticas):

1. Calcula la media de cada caracter√≠stica:

$$
\mu_j = \frac{1}{n} \sum_{i=1}^{n} X_{ij}
$$

2. Resta la media a cada columna:

$$
X_{\text{centrado}} = X - \text{media}(X)
$$

‚úÖ Este paso es **fundamental** porque PCA mide la **varianza** respecto al **origen** (0).

---

## 2.2 Construir la matriz de covarianza

Ahora, construimos la **matriz de covarianza** de los datos centrados:

$$
C = \frac{1}{n-1} X_{\text{centrado}}^T X_{\text{centrado}}
$$

- $C \in \mathbb{R}^{d \times d}$,
- $C_{ij}$ mide c√≥mo las variables $i$ y $j$ var√≠an juntas.

**Notas importantes:**
- $C$ es **sim√©trica** ($C^T = C$),
- $C$ es **semi-definida positiva** (todos sus autovalores son \(\geq 0\)).

---

## 2.3 El objetivo de PCA: maximizar la varianza proyectada

Ahora, formalizamos el objetivo de PCA:

**Queremos encontrar una direcci√≥n $w \in \mathbb{R}^d$ (un vector unitario) tal que:**

- La **varianza** de los datos proyectados sobre $w$ sea **m√°xima**.

Matem√°ticamente:

- Proyectar un dato $x_i$ sobre $w$ es:

$$
\text{proyecci√≥n} = w^T x_i
$$

- La varianza de todas las proyecciones es:

$$
\text{Var}(w^T X) = w^T C w
$$

‚úÖ **Queremos maximizar $w^T C w$ sujeto a que $||w|| = 1$ (norma 1).**

---

## 2.4 Problema de optimizaci√≥n

Planteamos el problema de optimizaci√≥n:

$$
\max_{w} \quad w^T C w
\quad \text{ sujeto a } \quad w^T w = 1
$$

Este es un problema cl√°sico de optimizaci√≥n con restricciones, que resolvemos usando **multiplicadores de Lagrange**.

### Construimos el lagrangiano:

$$
\mathcal{L}(w, \lambda) = w^T C w - \lambda (w^T w - 1)
$$

### Derivamos respecto a $w$ y igualamos a cero:

$$
\nabla_w \mathcal{L} = 2Cw - 2\lambda w = 0
$$

Dividiendo entre 2:

$$
Cw = \lambda w
$$

¬°Esto es la **ecuaci√≥n de autovalores**!

- **$w$** debe ser un **autovector** de $C$,
- **$\lambda$** es el **autovalor** correspondiente.

‚úÖ **Conclusi√≥n:**  
**La direcci√≥n que maximiza la varianza proyectada es el autovector de $C$ asociado al mayor autovalor.**

---

## 2.5 C√≥mo encontrar todos los componentes principales

Despu√©s de encontrar el **primer autovector** (m√°xima varianza), podemos seguir:

- El **segundo componente principal** es el autovector correspondiente al **segundo mayor autovalor**,
- Y as√≠ sucesivamente.

‚úÖ Cada componente principal:
- Es **ortogonal** a los anteriores (por la simetr√≠a de $C$),
- Maximiza la varianza en las direcciones restantes.

---

## 2.6 Interpretaci√≥n geom√©trica de los resultados

Cuando hacemos PCA:

- Rotamos el espacio de coordenadas a una nueva base ortonormal (los autovectores),
- En esa base, la dispersi√≥n de los datos est√° alineada con los ejes.

Cada nuevo eje:

- Captura un porcentaje de la varianza total,
- El porcentaje lo da su autovalor relativo al total de autovalores.

**Varianza explicada por el componente $i$:**

$$
\text{Varianza explicada}_i = \frac{\lambda_i}{\sum_{j=1}^{d} \lambda_j}
$$

---

## 2.7 Relaci√≥n entre PCA y SVD

Adem√°s, podemos hacer PCA usando la **Descomposici√≥n en Valores Singulares (SVD)** directamente:

Dado $X_{\text{centrado}}$, su SVD es:

$$
X_{\text{centrado}} = U \Sigma V^T
$$

- $V$ contiene los **autovectores** de $C$,
- Los **valores singulares** $\sigma_i$ en $\Sigma$ est√°n relacionados con los **autovalores** de $C$ as√≠:

$$
\lambda_i = \frac{\sigma_i^2}{n-1}
$$

‚úÖ Esto a veces es m√°s **num√©ricamente estable** que calcular la covarianza directamente.

---

# üìã En resumen (Secci√≥n 2)

Para hacer PCA manualmente:

1. **Centrar** los datos (restar la media).
2. **Construir** la matriz de covarianza $C$.
3. **Encontrar** los autovalores y autovectores de $C$.
4. **Ordenar** los autovectores seg√∫n autovalores de mayor a menor.
5. **Proyectar** los datos sobre los primeros autovectores para reducir dimensiones.

‚úÖ Cada paso tiene una justificaci√≥n matem√°tica basada en maximizar la varianza.

---
# üéì Minicurso de PCA - Secci√≥n 3: Aplicaci√≥n Manual a un Dataset (de 3D a 1D)

---

# ‚úèÔ∏è Paso 0: Definimos el Dataset

Usaremos este peque√±o dataset en $\mathbb{R}^3$:

| $x$ | $y$ | $z$ |
|:------:|:------:|:------:|
| 2.5 | 2.4 | 1.2 |
| 0.5 | 0.7 | 0.3 |
| 2.2 | 2.9 | 1.7 |
| 1.9 | 2.2 | 1.3 |
| 3.1 | 3.0 | 2.0 |
| 2.3 | 2.7 | 1.6 |
| 2.0 | 1.6 | 1.1 |
| 1.0 | 1.1 | 0.6 |
| 1.5 | 1.6 | 0.9 |
| 1.1 | 0.9 | 0.5 |

$X \in \mathbb{R}^{10 \times 3}$.

Objetivo: **Reducir de 3D a 1D usando PCA**.

---

# ‚úèÔ∏è Paso 1: Centrar los datos

Primero, calculamos la media de cada variable:

$$
\mu_x = \frac{2.5 + 0.5 + 2.2 + 1.9 + 3.1 + 2.3 + 2.0 + 1.0 + 1.5 + 1.1}{10} = \frac{18.1}{10} = 1.81
$$
$$
\mu_y = \frac{2.4 + 0.7 + 2.9 + 2.2 + 3.0 + 2.7 + 1.6 + 1.1 + 1.6 + 0.9}{10} = \frac{19.1}{10} = 1.91
$$
$$
\mu_z = \frac{1.2 + 0.3 + 1.7 + 1.3 + 2.0 + 1.6 + 1.1 + 0.6 + 0.9 + 0.5}{10} = \frac{11.2}{10} = 1.12
$$

Ahora restamos la media a cada valor para centrar los datos:

Ejemplo para la primera fila:

$$
(2.5, 2.4, 1.2) \quad \rightarrow \quad (2.5 - 1.81, 2.4 - 1.91, 1.2 - 1.12) = (0.69, 0.49, 0.08)
$$

Calculamos esto para cada fila (puedes usar una calculadora o hacerlo manualmente).

Resultado (datos centrados):

| $x$ | $y$ | $z$ |
|:------:|:------:|:------:|
| 0.69 | 0.49 | 0.08 |
| -1.31 | -1.21 | -0.82 |
| 0.39 | 0.99 | 0.58 |
| 0.09 | 0.29 | 0.18 |
| 1.29 | 1.09 | 0.88 |
| 0.49 | 0.79 | 0.48 |
| 0.19 | -0.31 | -0.02 |
| -0.81 | -0.81 | -0.52 |
| -0.31 | -0.31 | -0.22 |
| -0.71 | -1.01 | -0.62 |

‚úÖ Los datos ya est√°n centrados.

---

# ‚úèÔ∏è Paso 2: Construir la matriz de covarianza

La matriz de covarianza es:

$$
C = \frac{1}{n-1} X_{\text{centrado}}^T X_{\text{centrado}}
$$

donde $n = 10$, as√≠ que $n-1 = 9$.

Primero calculamos los productos cruzados.

Calculamos las entradas:

### Diagonal:
- $\text{Var}(x) = \frac{1}{9} \sum (\text{cada valor de x centrado})^2$
- $\text{Var}(y) = \frac{1}{9} \sum (\text{cada valor de y centrado})^2$
- $\text{Var}(z) = \frac{1}{9} \sum (\text{cada valor de z centrado})^2$

### Fuera de la diagonal:
- $\text{Cov}(x,y) = \frac{1}{9} \sum (\text{cada x centrado} \times \text{su y correspondiente})$
- $\text{Cov}(x,z)$
- $\text{Cov}(y,z)$

**C√°lculos:**

**Varianzas:**
- $$
\text{Var}(x) = \frac{1}{9}(0.69^2 + (-1.31)^2 + \dots + (-0.71)^2) = \frac{1}{9}(5.549) = 0.6165
$$
- $$
\text{Var}(y) = \frac{1}{9}(0.49^2 + (-1.21)^2 + \dots + (-1.01)^2) = \frac{1}{9}(5.559) = 0.6177
$$
- $$
\text{Var}(z) = \frac{1}{9}(0.08^2 + (-0.82)^2 + \dots + (-0.62)^2) = \frac{1}{9}(2.865) = 0.3183
$$

**Covarianzas:**
- $$
\text{Cov}(x,y) = \frac{1}{9}(0.69 \times 0.49 + (-1.31) \times (-1.21) + \dots + (-0.71) \times (-1.01)) = \frac{1}{9}(5.439) = 0.6043
$$
- $$
\text{Cov}(x,z) = \frac{1}{9}(0.69 \times 0.08 + (-1.31) \times (-0.82) + \dots + (-0.71) \times (-0.62)) = \frac{1}{9}(2.756) = 0.3062
$$
- $$
\text{Cov}(y,z) = \frac{1}{9}(0.49 \times 0.08 + (-1.21) \times (-0.82) + \dots + (-1.01) \times (-0.62)) = \frac{1}{9}(2.809) = 0.3121
$$

Entonces, la matriz de covarianza $C$ es:

$$
C = \begin{pmatrix}
0.6165 & 0.6043 & 0.3062 \\
0.6043 & 0.6177 & 0.3121 \\
0.3062 & 0.3121 & 0.3183
\end{pmatrix}
$$

---

# ‚úèÔ∏è Paso 3: Encontrar autovalores y autovectores

Ahora resolvemos:

$$
\det(C - \lambda I) = 0
$$

donde $I$ es la matriz identidad.

‚úÖ Este c√°lculo implica resolver un polinomio c√∫bico.  
Por simplicidad pr√°ctica, te doy el resultado:

Los autovalores son aproximadamente:

$$
\lambda_1 \approx 1.2840, \quad \lambda_2 \approx 0.0490, \quad \lambda_3 \approx 0.2195
$$

Ordenados de mayor a menor: $\lambda_1, \lambda_3, \lambda_2$.

Los autovectores correspondientes son (normalizados):

$$
v_1 = \begin{pmatrix} 0.6779 \\ 0.7330 \\ 0.0505 \end{pmatrix}
$$
$$
v_3 = \begin{pmatrix} 0.2354 \\ 0.1820 \\ -0.9541 \end{pmatrix}
$$
$$
v_2 = \begin{pmatrix} 0.6972 \\ -0.6621 \\ -0.2742 \end{pmatrix}
$$

---

# ‚úèÔ∏è Paso 4: Proyectar los datos sobre el primer componente principal

Queremos reducir de 3D ‚Üí 1D, as√≠ que solo usamos el primer autovector $v_1$.

Para cada dato centrado $x_i$:

$$
\text{proyecci√≥n}_i = v_1^T x_i
$$

‚úÖ Multiplicamos el vector fila del dato por el vector columna del autovector principal.

Ejemplo con el primer dato:

$$
(0.69, 0.49, 0.08) \times (0.6779, 0.7330, 0.0505)^T
$$
$$
= (0.69 \times 0.6779) + (0.49 \times 0.7330) + (0.08 \times 0.0505)
$$
$$
= 0.4678 + 0.3592 + 0.0040
$$
$$
= 0.8310
$$

‚úÖ Haces esto para los 10 datos, obteniendo 10 valores 1D.

---

# üß† Interpretaci√≥n

- Ahora cada punto 3D se convierte en un **n√∫mero real** (una coordenada 1D).
- Podr√≠as graficarlos:
  - Antes: nube de puntos en espacio 3D.
  - Despu√©s: puntos sobre una l√≠nea 1D.
- Visualmente, ver√≠as c√≥mo PCA **aplasta** los datos sobre la **direcci√≥n de m√°xima varianza**.

---

El c√≥digo del ejemplo con el paso a paso real se encuentra en [IA.ipynb - Colab](https://colab.research.google.com/drive/1Yc-LM3lQdKDnGElwUPTKRVaYPyXFeOyE#scrollTo=n5A2M7E3yyO4) (PCA)




