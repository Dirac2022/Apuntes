
# üéì Minicurso de **Sparse PCA (SPCA)**

## 1.1 ¬øPor qu√© Sparse PCA?

Ya sabes que **PCA** encuentra combinaciones lineales de variables originales que maximizan la varianza.
El problema es que:

* En PCA cl√°sico, **cada componente principal usa *todas*** las variables originales (con pesos distintos).
* Esto hace que los componentes sean **dif√≠ciles de interpretar**:

  * Ejemplo: el primer componente puede ser una mezcla de las 100 variables ‚Üí imposible explicar claramente qu√© significa.
* En √°reas como **biolog√≠a, neurociencia, miner√≠a de datos**, la **interpretabilidad** es clave.

üëâ **Sparse PCA** surge para resolver esto:

* Busca **componentes principales dispersos (sparse)**, es decir, que usen **pocas variables significativas**.
* As√≠, cada componente es m√°s **f√°cil de interpretar** y puede revelar **estructuras ocultas** en los datos.

---

## 1.2 Intuici√≥n de Sparse PCA

* En PCA normal:

  * Un componente es un vector $w \in \mathbb{R}^d$ con entradas **densas** (casi todas diferentes de 0).
* En **SPCA**:

  * Queremos que $w$ tenga **muchos ceros** ‚Üí solo unas pocas variables ‚Äúentran en juego‚Äù.

Visualmente:

* PCA: cada eje nuevo es una combinaci√≥n de todas las variables.
* SPCA: cada eje nuevo es una combinaci√≥n **s√≥lo de algunas variables relevantes**.

---

## 1.3 Formalizaci√≥n matem√°tica

Recordemos que PCA resolv√≠a:

$$
\max_{w} \; w^T C w \quad \text{sujeto a } \; \|w\|_2 = 1
$$

donde $C$ es la matriz de covarianza.

üëâ En SPCA a√±adimos una restricci√≥n de **sparsity (dispersi√≥n)**.
Existen varias formas, pero la m√°s com√∫n es a√±adir una penalizaci√≥n tipo **L1** (como en LASSO):

$$
\max_{w} \; w^T C w - \alpha \|w\|_1
\quad \text{sujeto a } \; \|w\|_2 = 1
$$

* $\|w\|_1 = \sum |w_i|$ ‚Üí fomenta que muchos pesos sean **exactamente cero**.
* $\alpha > 0$ ‚Üí controla cu√°nta dispersi√≥n imponemos:

  * $\alpha = 0$: recuperamos PCA cl√°sico,
  * $\alpha$ grande: obtenemos vectores muy escasos, pero quiz√° con menor varianza capturada.

---

## 1.4 Diferencias clave entre PCA y SPCA

| Aspecto                      | PCA cl√°sico                                | Sparse PCA                                          |
| ---------------------------- | ------------------------------------------ | --------------------------------------------------- |
| **Varianza explicada**       | M√°xima posible                             | Algo menor (se sacrifica un poco)                   |
| **Combinaci√≥n de variables** | Usa todas las variables                    | Usa pocas (sparse)                                  |
| **Interpretaci√≥n**           | Dif√≠cil: cada componente es mezcla de todo | F√°cil: cada componente involucra pocas variables    |
| **Aplicaci√≥n pr√°ctica**      | Reducci√≥n de dimensionalidad ‚Äút√©cnica‚Äù     | Interpretaci√≥n y selecci√≥n de variables importantes |

---

## 1.5 M√©todos para resolver SPCA

Resolver SPCA es m√°s complejo que PCA (ya no basta con autovectores).
Existen varios enfoques:

1. **Penalizaci√≥n L1 (LASSO-like):**

   * Se resuelve con t√©cnicas de optimizaci√≥n convexa.
   * Implementado en scikit-learn (`SparsePCA`).

2. **Penalizaci√≥n combinatoria:**

   * Restringir expl√≠citamente el n√∫mero de variables no nulas en $w$.
   * M√°s dif√≠cil de optimizar (problema NP-hard).

3. **M√©todos de rotaci√≥n (varimax, etc.):**

   * Tras PCA, se aplican transformaciones para hacer m√°s ‚Äúsparse‚Äù los componentes.

---

## 1.6 Interpretaci√≥n en miner√≠a de datos

Sparse PCA no solo reduce dimensionalidad, tambi√©n act√∫a como una forma de **selecci√≥n de caracter√≠sticas**:

* Identifica **qu√© variables importan m√°s** en cada componente.
* √ötil cuando:

  * Tenemos muchas variables correlacionadas,
  * Queremos explicar fen√≥menos en t√©rminos claros (ej. en biolog√≠a: genes relevantes).

Ejemplo intuitivo:

* Dataset con 1000 genes ‚Üí PCA mezcla todos.
* SPCA ‚Üí cada componente podr√≠a estar relacionado con un subconjunto peque√±o de genes ‚Üí m√°s interpretable.

---

## 1.7 Ejemplo sencillo (conceptual)

Sup√≥n que tienes 5 variables: $x_1, x_2, x_3, x_4, x_5$.

* **PCA cl√°sico** podr√≠a dar un primer componente:

$$
v_1 = (0.45, 0.43, 0.40, 0.42, 0.47)
$$

üëâ Dif√≠cil de interpretar, todas pesan parecido.

* **SPCA** podr√≠a dar:

$$
v_1 = (0.70, 0.65, 0, 0, 0)
$$

üëâ Mucho m√°s claro: el componente principal depende sobre todo de $x_1, x_2$.
(Quiz√° revelando que esas dos variables describen la variaci√≥n principal).

---

## 1.8 Relaci√≥n con PCA cl√°sico

* SPCA **no reemplaza** PCA, sino que lo **extiende**.
* Cuando no hay restricci√≥n de dispersi√≥n ($\alpha = 0$), SPCA = PCA.
* Piensa en PCA como **"m√°xima compresi√≥n"** y en SPCA como **"compresi√≥n interpretable"**.

---

# üìã En resumen

1. **Problema de PCA:** dif√≠cil de interpretar porque los componentes combinan todas las variables.
2. **Soluci√≥n:** Sparse PCA a√±ade restricciones de dispersi√≥n (L1) ‚Üí componentes con pocos pesos no nulos.
3. **Ventaja:** m√°s interpretabilidad, √∫til para selecci√≥n de variables y an√°lisis exploratorio.
4. **Costo:** se pierde algo de varianza explicada.
5. **Implementaci√≥n:** se resuelve con optimizaci√≥n convexa ‚Üí disponible en librer√≠as como scikit-learn.

---

¬øQuieres que te prepare tambi√©n un **ejemplo pr√°ctico paso a paso** con un dataset peque√±o (tipo el de 3D ‚Üí 1D que ya hiciste en PCA) pero ahora comparando PCA vs SPCA, para que veas c√≥mo cambia la interpretabilidad?
