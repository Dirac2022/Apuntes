
**Resumen**  
¬øC√≥mo podemos realizar inferencia y aprendizaje eficientes en modelos probabil√≠sticos dirigidos, en presencia de variables latentes continuas con distribuciones posteriores intratables y grandes conjuntos de datos? Introducimos un algoritmo de inferencia variacional estoc√°stica y aprendizaje que escala a grandes conjuntos de datos y, bajo algunas condiciones leves de diferenciabilidad, incluso funciona en el caso intratable. Nuestras contribuciones son dos. Primero, mostramos que una reparametrizaci√≥n de la cota inferior variacional produce un estimador de la cota inferior que puede optimizarse de manera directa usando m√©todos est√°ndar de gradiente estoc√°stico. Segundo, mostramos que para conjuntos de datos i.i.d. con variables latentes continuas por punto de dato, la inferencia posterior puede hacerse especialmente eficiente ajustando un modelo de inferencia aproximado (tambi√©n llamado modelo de reconocimiento) a la distribuci√≥n posterior intratable usando el estimador de cota inferior propuesto. Las ventajas te√≥ricas se reflejan en los resultados experimentales.

Perfecto üôå. Vamos con la **traducci√≥n fiel** y luego con la **explicaci√≥n breve y directa**.

---


**1 Introducci√≥n**  
¬øC√≥mo podemos realizar inferencia aproximada y aprendizaje eficientes con modelos probabil√≠sticos dirigidos cuyas variables latentes continuas y/o par√°metros tienen distribuciones posteriores intratables? El enfoque bayesiano variacional (VB) implica la optimizaci√≥n de una aproximaci√≥n a la distribuci√≥n posterior intratable. Desafortunadamente, el enfoque com√∫n de _mean-field_ requiere soluciones anal√≠ticas de expectativas con respecto a la distribuci√≥n posterior aproximada, las cuales tambi√©n son intratables en el caso general.

Mostramos c√≥mo una reparametrizaci√≥n de la cota inferior variacional produce un estimador simple, diferenciable y no sesgado de la cota inferior; este estimador SGVB (_Stochastic Gradient Variational Bayes_) puede usarse para inferencia aproximada eficiente de la distribuci√≥n posterior en casi cualquier modelo con variables latentes continuas y/o par√°metros, y puede optimizarse f√°cilmente usando t√©cnicas est√°ndar de ascenso por gradiente estoc√°stico.

Para el caso de un conjunto de datos i.i.d. y variables latentes continuas por cada punto de dato, proponemos el algoritmo **Auto-Encoding VB (AEVB)**. En el algoritmo AEVB hacemos que la inferencia y el aprendizaje sean especialmente eficientes al usar el estimador SGVB para optimizar un modelo de reconocimiento que nos permite realizar una inferencia posterior aproximada muy eficiente utilizando un simple muestreo ancestral, lo que a su vez nos permite aprender eficientemente los par√°metros del modelo, sin necesidad de esquemas de inferencia iterativos costosos (como MCMC) por cada punto de dato.

El modelo de inferencia posterior aproximado aprendido tambi√©n puede usarse para una gran cantidad de tareas tales como reconocimiento, eliminaci√≥n de ruido (_denoising_), representaci√≥n y prop√≥sitos de visualizaci√≥n. Cuando se usa una red neuronal para el modelo de reconocimiento, llegamos al **autoencoder variacional (VAE)**.

---

### ü§î Explicaci√≥n breve y directa

1. **Problema**:
    
    - Queremos hacer inferencia en modelos probabil√≠sticos con variables ocultas continuas.
        
    - El problema: la **posterior es intratable** (no se puede calcular exactamente).
        
2. **Soluci√≥n cl√°sica (mean-field VB)**:
    
    - Aproximamos la posterior con una distribuci√≥n simple.
        
    - Pero a√∫n as√≠, muchas veces requiere c√°lculos anal√≠ticos imposibles.
        
3. **Lo que proponen**:
    
    - Usar una **reparametrizaci√≥n** para obtener un estimador que:
        
        - Sea **no sesgado** (no distorsiona la expectativa).
            
        - Sea **diferenciable** (podemos usar gradientes).
            
        - Funcione con gradiente estoc√°stico, igual que entrenar redes neuronales.
            
4. **AEVB**:
    
    - Combina SGVB + un **modelo de reconocimiento (encoder)** para aproximar la posterior.
        
    - Ya no necesitas usar m√©todos lentos como MCMC para cada dato.
        
    - Si el modelo de reconocimiento es una red neuronal ‚Üí tienes el **VAE**.
        

---

üëâ Ejemplo directo:  
Antes, para cada imagen en MNIST ten√≠as que correr un proceso de inferencia largo (como MCMC).  
Con **AEVB**, basta pasar la imagen por una red (el encoder), obtener una distribuci√≥n para zz, y entrenar con gradiente estoc√°stico ‚Üí mucho m√°s r√°pido y escalable.

---

¬øQuieres que en la siguiente parte te traduzca y explique la **secci√≥n 2: Background**?